{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f11bb8c-0dff-4842-87a9-86b856e382db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f69010-57e2-4f57-93af-3366952c0164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "\u001b[1m527605760/553467096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m6s\u001b[0m 0us/step"
     ]
    }
   ],
   "source": [
    "#DISPLAYED MODEL SUMMARY TO UNDERSTAND THE ARCHITECTURE\n",
    "model = VGG16()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe6b48-bff7-4ca5-a095-465572665cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622cd7ee-9c94-41d4-b01f-2d606f02636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img \n",
    "#This function loads an image from a specified file path. \n",
    "#You can specify the image size (e.g., target_size=(224, 224)) to match the input dimensions required by the VGG16 model (224x224 pixels).\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "#This converts the loaded image into a NumPy array, allowing it to be processed by machine learning models.\n",
    "#The resulting array represents the pixel values of the image in three color channels (RGB)\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "#This function prepares the image data for use with the VGG16 model by applying the necessary preprocessing steps \n",
    "#(e.g., scaling and mean subtraction based on the ImageNet dataset). \n",
    "#It adjusts the pixel values to the format expected by VGG16\n",
    "\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "#This function converts the model’s output (a probability distribution) into human-readable class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cdf47b-0c33-4c00-8382-aebc3260400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an image from file\n",
    "image = load_img('test.jpeg', target_size=(224, 224))\n",
    "# convert the image pixels to a numpy array\n",
    "image = img_to_array(image)\n",
    "# reshape data for the model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "# prepare the image for the VGG model\n",
    "image = preprocess_input(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca3269c-051c-4483-b30b-e198469ccc15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
